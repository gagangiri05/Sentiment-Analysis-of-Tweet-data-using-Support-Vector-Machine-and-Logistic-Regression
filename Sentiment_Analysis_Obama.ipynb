{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee24d3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\agupt69\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "from gensim.models import Doc2Vec, doc2vec\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "nltk.download('stopwords')\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a095ebf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1            1922\n",
      "0             1896\n",
      "1             1653\n",
      "2             1474\n",
      "0               82\n",
      "2               70\n",
      "-1              46\n",
      "1               26\n",
      "irrevelant      23\n",
      "irrelevant       1\n",
      "Name: classlabel, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read the data from the excel file\n",
    "xls = pd.ExcelFile('training-Obama-Romney-tweets.xlsx')\n",
    "df = pd.read_excel(xls, 'Obama')\n",
    "# drop the first row from the data \"1: positive, -1: negative, 0: neutral, 2: mixed\"\n",
    "df = df.drop([0])\n",
    "# Rename the unnamed column 4 that contains the class\n",
    "df.rename(columns={'Unnamed: 4':'classlabel'}, inplace=True )\n",
    "# print the unique class label inorder to prepare for pre processing\n",
    "print(df['classlabel'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d40fb317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all the unnecessary and empty columns\n",
    "df = df[['Anootated tweet', 'classlabel']]\n",
    "# Drop all the rows where the class is mixed i.e 2\n",
    "df = df.dropna(axis=0)\n",
    "# The data provided had class 'irrelevant' and 'irrevelant', so dropping those\n",
    "df = df[df.classlabel != 'irrelevant']\n",
    "df = df[df.classlabel != 'irrevelant']\n",
    "# Converting the classlabel column to type integer\n",
    "df['classlabel'] = df['classlabel'].astype(int)\n",
    "# Dropping all rows that have been labelled as mixed class, i.e. class (2)\n",
    "df = df[df.classlabel != 2]\n",
    "#df.loc[(df.classlabel == -1),'classlabel']= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "439c1b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0    1977\n",
      "-1    1968\n",
      " 1    1679\n",
      "Name: classlabel, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the unique values again to make that the data only consists of positive (1), negative(-1) and neutral(0) classes\n",
    "print(df['classlabel'].value_counts())\n",
    "# Extract all tweets and put them into a list for cleaning them up\n",
    "tweets = df['Anootated tweet'].tolist()\n",
    "class_tweets = df['classlabel'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d8c8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets clean up\n",
    "def tweets_cleanup(tweet):\n",
    "    # Remove HTML tags from the tweet\n",
    "    tweet = re.sub(\"<.*?>\", \"\", tweet)\n",
    "    # Remove Twitter usernames from the tweet\n",
    "    tweet = re.sub(\"@[A-Za-z0-9_]+\",\"\", tweet)\n",
    "    # Remove hashtags from the tweet\n",
    "    tweet = re.sub(\"#[A-Za-z0-9_]+\",\"\", tweet)\n",
    "    # Remove http/https link from the tweet\n",
    "    tweet = re.sub(r'http\\S+', \"\", tweet)\n",
    "    # Remove special characters\n",
    "    #tweet = re.sub(\"[<>!#@$:.,%\\?(-)]+\", \"\", tweet)\n",
    "    tweet = re.sub('[^a-zA-Z\\d\\s\\n]', \"\", tweet)\n",
    "    # Remove all numbers from the tweet\n",
    "    tweet = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", tweet)\n",
    "    # Convert tweet to lower case\n",
    "    tweet = tweet.lower()\n",
    "    # Remove extra white spaces from start and end of tweet\n",
    "    tweet = tweet.strip()\n",
    "    # Removing stop words and applying stemming to the tweets\n",
    "    words = tweet.split()\n",
    "    tweet = ' '.join([w for w in words if not w in nltk.corpus.stopwords.words(\"english\")])\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    stemmedTweet = [ps.stem(word) for word in tweet.split(\" \")]\n",
    "    stemmedTweet = \" \".join(stemmedTweet)\n",
    "    tweet = str(stemmedTweet)\n",
    "    tweet = tweet.replace(\"'\", \"\")\n",
    "    tweet = tweet.replace(\"\\\"\",\"\")\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aca8b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(predictions):\n",
    "    with open('obama.txt', 'w') as f:\n",
    "        i = 1\n",
    "        f.write(\"Ayush 76 and Gagan 82 \\n\")\n",
    "        for val in predictions:\n",
    "            line = str(i)+\";;\"+str(val)\n",
    "            f.write(line)\n",
    "            i = i + 1\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "311f32d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5624, 3000)\n"
     ]
    }
   ],
   "source": [
    "# Apply the clean up function to the tweets\n",
    "df['Cleaned tweet'] = df['Anootated tweet'].apply(tweets_cleanup)\n",
    "df.to_excel(\"cleaned_tweets.xlsx\")  \n",
    "X = df['Cleaned tweet']\n",
    "Y = df['classlabel']\n",
    "vectorizer = CountVectorizer(max_features=3000).fit(X)\n",
    "X = vectorizer.transform(X)\n",
    "print(X.shape)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e02030a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Accuracy 0.5742222222222222\n",
      "SGD F1 score 0.5757455731067996\n",
      "SGD Precision 0.5770282722401543\n",
      "SGD Recall 0.5749138422300367\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent\n",
    "sgd = SGDClassifier()\n",
    "x_train.shape\n",
    "y_train.shape\n",
    "sgd.fit(x_train, y_train)\n",
    "sgd_predictions = sgd.predict(x_test)\n",
    "print(\"SGD Accuracy\", accuracy_score(y_test, sgd_predictions))\n",
    "print(\"SGD F1 score\", metrics.f1_score(y_test, sgd_predictions, average='macro'))\n",
    "print(\"SGD Precision\", metrics.precision_score(y_test, sgd_predictions, average='macro'))\n",
    "print(\"SGD Recall\", metrics.recall_score(y_test, sgd_predictions, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd04117a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy 0.5973333333333334\n",
      "SVM F1 score 0.5995949622195567\n",
      "SVM Precision 0.606929319288065\n",
      "SVM Recall 0.596409180034528\n"
     ]
    }
   ],
   "source": [
    "# SVM \n",
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='rbf')\n",
    "svclassifier.fit(x_train, y_train)\n",
    "svm_pred = svclassifier.predict(x_test)\n",
    "print(\"SVM Accuracy\", accuracy_score(y_test, svm_pred))\n",
    "print(\"SVM F1 score\", metrics.f1_score(y_test, svm_pred, average='macro'))\n",
    "print(\"SVM Precision\", metrics.precision_score(y_test, svm_pred, average='macro'))\n",
    "print(\"SVM Recall\", metrics.recall_score(y_test, svm_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c6f73cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Accuracy 0.5973333333333334\n",
      "NB F1 score 0.5995949622195567\n",
      "NB Precision 0.606929319288065\n",
      "NB Recall 0.596409180034528\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(x_train, y_train)\n",
    "nb_pred = svclassifier.predict(x_test)\n",
    "print(\"NB Accuracy\", accuracy_score(y_test, nb_pred))\n",
    "print(\"NB F1 score\", metrics.f1_score(y_test, nb_pred, average='macro'))\n",
    "print(\"NB Precision\", metrics.precision_score(y_test, nb_pred, average='macro'))\n",
    "print(\"NB Recall\", metrics.recall_score(y_test, nb_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19bb3a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy 0.44177777777777777\n",
      "XGBoost F1 score 0.37557380675847574\n",
      "XGBoost Precision 0.6180101670297748\n",
      "XGBoost Recall 0.42676742866688827\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Model\n",
    "reg = xgb.XGBRegressor()\n",
    "reg.fit(x_train, y_train)\n",
    "pred = reg.predict(x_test)\n",
    "xg_pred = list()\n",
    "for p in pred:\n",
    "    p = round(p)\n",
    "    xg_pred.append(p)\n",
    "print(\"XGBoost Accuracy\", accuracy_score(y_test, xg_pred))\n",
    "print(\"XGBoost F1 score\", metrics.f1_score(y_test, xg_pred, average='macro'))\n",
    "print(\"XGBoost Precision\", metrics.precision_score(y_test, xg_pred, average='macro'))\n",
    "print(\"XGBoost Recall\", metrics.recall_score(y_test, xg_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f61e737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy 0.6026666666666667\n",
      "Logistic Regression F1 score 0.6026666666666667\n",
      "Logistic Regression Precision 0.6026666666666667\n",
      "Logistic Regression Recall 0.6026666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agupt69\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(x_train, y_train)\n",
    "lr_pred = logisticRegr.predict(x_test)\n",
    "print(\"Logistic Regression Accuracy\", accuracy_score(y_test, lr_pred))\n",
    "print(\"Logistic Regression F1 score\", metrics.f1_score(y_test, lr_pred, average='macro'))\n",
    "print(\"Logistic Regression Precision\", metrics.precision_score(y_test, lr_pred, average='macro'))\n",
    "print(\"Logistic Regression Recall\", metrics.recall_score(y_test, lr_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a5465508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1951, 3600)\n"
     ]
    }
   ],
   "source": [
    "# Test data with SVM for Obama\n",
    "test_xls = pd.ExcelFile('final-testData-no-label-Obama-tweets(3).xlsx')\n",
    "test_df = pd.read_excel(test_xls, 'Obama')\n",
    "# Drop all the rows where the class is mixed i.e 2\n",
    "#test_df = test_df['Anootated tweet']\n",
    "#test_df = test_df.dropna(axis=0)\n",
    "test_df.rename(columns={'Unnamed: 1':'tweet'}, inplace=True )\n",
    "test_df['Cleaned tweet'] = test_df['tweet'].apply(tweets_cleanup)\n",
    "X_testing = test_df['Cleaned tweet']\n",
    "vectorizer = CountVectorizer(max_features=3600).fit(X_testing)\n",
    "X_testing = vectorizer.transform(X_testing)\n",
    "print(X_testing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c7f8c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = logisticRegr.predict(X_testing)\n",
    "write_to_file(final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d39debf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
